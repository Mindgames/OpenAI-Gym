{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LunarLandercontinues-v2-PPO-GymLeaderboard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# **OpenAI Gym LunarLandercontinues-v2 PPO** \n",
        "\n",
        "This colab showcase the training and evaluation of model on LunarLanderContinues-v2 enviorment. For more information visit [Git repo](https://github.com/nextgrid/deep-learning-labs-openAI)\n",
        "\n",
        "\n",
        "\n",
        "### **Links**\n",
        "[Nextgrid](https://nextgrid.ai)  \n",
        "[Deep Learning Labs](https://nextgrid.ai/dll)  \n",
        "\n",
        "----\n",
        "\n",
        "[![Nextgrid Artificial Intelligence](https://storage.googleapis.com/nextgrid_github_repo_visuals/Github%20Graphics%20/big-banner.jpg)](https://nextgrid.ai)\n",
        "\n",
        " \n",
        "### **Nextgrid** - _The **Superlative** destination for AI-first startups & talent_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️▪️️️️️️️  \n",
        "*Notebook by Mathias*  \n",
        "*I would love your feedback,*  \n",
        "*or discuss your DL/DRL startup/business idea.*   \n",
        "*find me on* _[twitter](https://twitter.com/mathiiias123)_ or _[linkedin](https://www.linkedin.com/in/imathias)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22TBe2qeFlyr"
      },
      "source": [
        "## Install system wide packages\n",
        "Install linux server packages using `apt-get` and Python packages using `pip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install swig cmake python3-dev libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg #remove -qq for full output\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "!pip install torch stable-baselines3[extra,tests,docs]>=0.10.0 box2d box2d-kengz pyvirtualdisplay pyglet==1.5.0 --quiet #remove --quiet for full output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Dependencis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pToLfvOzCKQ"
      },
      "source": [
        "import gym\n",
        "import imageio\n",
        "import time\n",
        "import numpy as np\n",
        "import base64\n",
        "import IPython\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Video \n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Stable baselines\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "\n",
        "# Stable baselines 3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import CallbackList, BaseCallback, CheckpointCallback, EveryNTimesteps, \\\n",
        "    EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnMaxEpisodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKA52SBe6JdJ"
      },
      "source": [
        "### Variables\n",
        "env_id = 'LunarLanderContinuous-v2'\n",
        "video_folder = '/videos'\n",
        "video_length = 5000\n",
        "logs_base_dir = './runs' # Log DIR\n",
        "steps_total= 0 # Keep track of total steps\n",
        "time_steps = 2000000\n",
        "reward_threshold = 200\n",
        "episodes_threshold = 1000\n",
        "episodes = 0\n",
        "mean_reward = 0\n",
        "\n",
        "### Set log dir\n",
        "os.makedirs(logs_base_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "### Enviorment \n",
        "env = gym.make(env_id)\n",
        "env = Monitor(env, logs_base_dir)\n",
        "score = 0\n",
        "log_interval = 10          # Print avg reward after interval\n",
        "\n",
        "\n",
        "### Hyperparameters \n",
        "\n",
        "hp = {'activation_fn': 'relu', 'batch_size': 8, 'clip_range': 0.4, \n",
        "      'ent_coef': 2.89108e-05, 'gae_lambda': 0.92, 'gamma': 0.99, 'log_std_init': -0.00775684,\n",
        "      'lr': 0.000242873, 'max_grad_norm': 0.3, 'net_arch': 'medium',\n",
        "      'n_epochs': 10, 'n_steps': 1024, 'ortho_init': True, 'sde_sample_freq': 8, 'vf_coef': 0.856625}\n",
        "\n",
        "\n",
        "model = PPO(\n",
        "    MlpPolicy,\n",
        "    env,\n",
        "    n_steps=hp[\"n_steps\"],\n",
        "    batch_size=hp[\"batch_size\"],\n",
        "    gamma=hp[\"gamma\"],\n",
        "    learning_rate=hp[\"lr\"],\n",
        "    ent_coef=hp[\"ent_coef\"],\n",
        "    clip_range=hp[\"clip_range\"],\n",
        "    n_epochs=hp[\"n_epochs\"],\n",
        "    gae_lambda=hp[\"gae_lambda\"],\n",
        "    max_grad_norm=hp[\"max_grad_norm\"],\n",
        "    vf_coef=hp[\"vf_coef\"],\n",
        "    sde_sample_freq=hp[\"sde_sample_freq\"],\n",
        "    policy_kwargs=dict(\n",
        "        log_std_init=hp[\"log_std_init\"],\n",
        "        net_arch=[dict(pi=[128, 128], vf=[128, 128])],\n",
        "        activation_fn=nn.LeakyReLU,\n",
        "        ortho_init=hp[\"ortho_init\"],\n",
        "\n",
        "    ),\n",
        "    verbose=0\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qcuGpFzhIZY"
      },
      "source": [
        "## Record & display video\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-qEyqYl86uI"
      },
      "source": [
        "### Record & Display Video\n",
        "\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "# Record video\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  env = VecVideoRecorder(env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  env.close()\n",
        "\n",
        "\n",
        "## Display video\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PP-0aa90TNz"
      },
      "source": [
        "# RewardCallback function\n",
        "Handles all the evaluation during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atFFriQ-r6zI"
      },
      "source": [
        "class RewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "    It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(RewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # Retrieve training reward\n",
        "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "            if len(x) > 0:\n",
        "                global episodes\n",
        "                global mean_reward\n",
        "                episodes = len(y)\n",
        "                # print(episodes)\n",
        "                mean_reward = np.mean(y[-10:])\n",
        "                mean_reward = round(mean_reward, 0)\n",
        "                if self.verbose > 0:\n",
        "                    print(f\"Episodes: {episodes}\")\n",
        "                    print(f\"Num steps: {self.num_timesteps}\")\n",
        "                    print(f\"Mean reward: {mean_reward:.2f} \")\n",
        "                    print(\"=========== NEXTGRID.AI ================\")\n",
        "                # Report intermediate objective value to Optima and Handle pruning\n",
        "                # trial.report(episodes, self.num_timesteps)\n",
        "                # if trial.should_prune():\n",
        "                #     raise optuna.TrialPruned()\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if episodes > episodes_threshold:\n",
        "                    print(\"Reward threshold achieved\")\n",
        "                    return False\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward > reward_threshold:\n",
        "                    model.save(\"ppo_lunarlandercontinues\")\n",
        "                    # record(name=steps_total, length=1750)\n",
        "                    # ep100 = evaluate_policy(model, eval_env, n_eval_episodes=50, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
        "                    # print(\"Mean Reward 100 Epispodes: \", ep100[0])\n",
        "\n",
        "                    # print(\"<======SCORE======>\")\n",
        "                    # print(score)\n",
        "                    # if score > reward_threshold:\n",
        "\n",
        "                    print(\"Model saved\")\n",
        "                    return False\n",
        "\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecisEsrX0dKu"
      },
      "source": [
        "callback = RewardCallback(check_freq=1000, log_dir=logs_base_dir)\n",
        "model.learn(total_timesteps=int(time_steps), callback=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odLPx9ogu31Z"
      },
      "source": [
        "# Evaluation\n",
        "OpenAI scores is generally messured over 100 epochs. Use code belowe to messure your avarage score over 100 rounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7D2_Fjzf935"
      },
      "source": [
        "evals = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
        "print(\"Score over 100 episodes\", evals[0])\n",
        "print(np.mean(evals[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfk5JmejMDoB"
      },
      "source": [
        "record_video(env_id, model, video_length=4000, prefix=\"name\")\n",
        "show_videos('videos', prefix=\"name\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGsuEeAfUBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAVOQYwTeAF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}